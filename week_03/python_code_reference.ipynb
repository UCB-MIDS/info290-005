{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction \n",
    "In this code we're going to wok through an extended example that demonstrates how randomization inference works. As noted in the lecture, we are working through the example of a randomized assignment of men and women to eat (or not eat) soybeans and we are measuring the level of estrogen present in each of their bloodstreams (perhaps in parts/million).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make Data \n",
    "\n",
    "First, we create a grouping variable with two groups, one called \"Man\", and another called \"Woman\". \n",
    "\n",
    "> Alex apologizes that David couldn't come up with an example that is either more interesting or less gender-binary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'group': np.repeat(a=['man', 'woman'], repeats=20)})\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To these groups, we assign silly, but schematically heplful *potential outcomes* to treatment and control. We say that, by some chance, we sampled men into our study that had estrogen ppm levels that ranged from 1-20, in perfect increments. Also, what luck, but we sampled women into the study that had estrogen levels that ranged from 51-70. So, the women have on average higher estrogen beginning the study. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['po_control'] = np.concatenate([np.arange(1,21), np.arange(51,71)], axis=None)\n",
    "df['po_treat'] = df['po_control'] # no effect because potential outcomes are the same "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we're going to use this later, let's wrap this *universe* creation into a function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_data(effect_size, number_per_group): \n",
    "    '''\n",
    "    This dataframe is going to be hardcoded to have 40 folks, 20 \"M\" and 20 \"F\". \n",
    "    The only argument is the magnitude of the difference between po_control and _treat\n",
    "    '''\n",
    "    \n",
    "    df = pd.DataFrame({'group': np.repeat(a=['man', 'woman'], repeats=number_per_group)})\n",
    "    df['po_control'] = np.concatenate(\n",
    "        [np.arange(1,number_per_group+1), \n",
    "         np.arange(1,number_per_group+1)+50],\n",
    "        axis=None)\n",
    "    df['po_treat'] = df['po_control'] + effect_size\n",
    "    \n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUMBER_PER_GROUP=200\n",
    "\n",
    "df = make_data(effect_size=0, number_per_group=NUMBER_PER_GROUP)\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the data is built, it is then we can start to consider actually interviening in the world to conduct the experiment. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conduct Experiment \n",
    "\n",
    "Per our randomization scheme, we are going to randomly assign the individuals to either: \n",
    "\n",
    "- Eat lots of tofu `(treatment == 1)`; or, \n",
    "- Not eat lots of tofu `(treatment == 0)`. \n",
    "\n",
    "To do this, we write a simple function that will randomly place zeros and ones for the treatment and control. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomize(number_per_group): \n",
    "    urn = np.repeat([0,1], repeats=number_per_group)\n",
    "    np.random.shuffle(urn)\n",
    "    return(urn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our randomization function in hand, we can now \"**run an experiment**\" which is going to: \n",
    "\n",
    "1. Assign people to receive treatment or control; \n",
    "2. If they receive treatment, we will see their potential outcomes to treatment; and, \n",
    "3. If they're in control, we will see their potential outcomes to control. \n",
    "\n",
    "Functionally, this is simply storing the results of our function randomize in a vector object called treatment, and then recording the correct potential outcome value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(_df, number_per_group, science_table): \n",
    "    '''\n",
    "    This takes an argument number per group, and an argument for\n",
    "    whether to return the science table (if True) or the observed\n",
    "    data (if False).\n",
    "    '''\n",
    "    _df['treat'] = randomize(number_per_group)\n",
    "    \n",
    "    _df['Y'] = np.NaN\n",
    "    _df.loc[df['treat']==0, 'Y'] = df.loc[df['treat']==0, 'po_control']\n",
    "    _df.loc[df['treat']==1, 'Y'] = df.loc[df['treat']==1, 'po_treat']\n",
    "\n",
    "    if science_table==False: \n",
    "        _df = _df[['group', 'treat', 'Y']]\n",
    "    \n",
    "    elif science_table==True: \n",
    "        _df = _df\n",
    "        \n",
    "    return(_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ = run_experiment(df, NUMBER_PER_GROUP, True)\n",
    "df_.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that we are setting up an experiment that has **no** effect. As Green and Gerber point out in *Field Experiments* in the case of the sharp-null, we are actually testing against the possiblity that we observe both the potential outcoems! Cool.\n",
    "\n",
    "Next, we create a vector of realized outcomes, first using the compact notation that Green and Gerber prefer using maths. For those randomized to treamtent, we multiply the potential outcome to treatment time the treatment vector (which is a 1 when they were assigned to treatment), and for those in control, into this vector we assign the potential outcome to control time the quantity `(1 - treatment)` which will be one when they are in the treatment group. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ok so we've got our data set up. Now what? \n",
    "Now that we have the data set up, we can begin to examine what the lecture is really about, what is the distribution of ATE that we observe due to the different possibly assignments to treatment and control. A few points to remember: \n",
    "\n",
    "1. From last week: The difference in sample means between the treatment and control groups is an unbiased estimator of the ATE. \n",
    "2. Also from last week: This does not imply that any one realization of treatment/control assignment is guaranteed to exactly produce that ATE.\n",
    "\n",
    "This is the entire point of understanding the distribution of the ATE. \n",
    "\n",
    "To get here, first lets write another function that will calculate the ATE. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_ate(_df): \n",
    "    '''This takes a df with names treat and Y.'''\n",
    "    res = _df[['treat', 'Y']].groupby('treat').mean()\n",
    "    ate = res.diff()['Y'][1]\n",
    "    \n",
    "    return(ate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ate = estimate_ate(_df = df_)\n",
    "ate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What gives?!\n",
    "\n",
    "We created this data such that there is *exactly* zero treament effect, but when we look at this summary of the outcomes, they're not **exactly** the same! *Shouldn't they have been?* \n",
    "\n",
    "Note, even more specifically than creating the data so that there was no *average treatment effect* we constructed this so that there was no effect at all -- for every single person, `po_control == po_treat`. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happens if you go back to the top and run the experiment again? Are the numbers internally consistent, even if they're producing a result that doesn't seem to make sense? Go back up and look at these again. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question of Understanding \n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    "Why are these values different each time you run your experiment? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Answer here. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More Work with this \n",
    "\n",
    "Lets rewrite our function just slightly so that rather than pulling the results frame, instead we're pulling the estimated average treatment effect from the experiment. This is just going to index into the values, and compute the difference between them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ri_sim_experiment(_df, number_per_group): \n",
    "    _df['treat'] = randomize(number_per_group)\n",
    "    res = _df[['treat', 'Y']].groupby('treat').mean()\n",
    "    res = res.diff()\n",
    "    return(res.iloc[1]['Y'].astype('int'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f'The ate under the sharp null is: {ri_sim_experiment(df_, NUMBER_PER_GROUP)}.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f'The ate under the sharp null is: {ri_sim_experiment(df_, NUMBER_PER_GROUP)}.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f'The ate under the sharp null is: {ri_sim_experiment(df_, NUMBER_PER_GROUP)}.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we're seeing is that there are some slightly different outcomes to different **randomization inference treatment regimes**. \n",
    "\n",
    "We can do this, a bunch more times using some simple list comprehension. Specifically, do this 1,000 times to get a sense of the distribution. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments = [ri_sim_experiment(df_, NUMBER_PER_GROUP) for _ in range(0,1000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "plt.hist(experiments)\n",
    "plt.axvline(x=ate, color = 'darkorange')\n",
    "plt.axvline(x = -1 * ate, color = 'darkorange')\n",
    "plt.title('Histogram of RI ATES')\n",
    "plt.xlabel('RI ATE')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it turns out, that was pretty similar to what we saw in our draw! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In fact, what we've got here is pretty likely to turn up by chance. \n",
    "\n",
    "Following David's statment, we can draw a probability of seeing an ATE of a given size under the repeated randomization regime. \n",
    "\n",
    "For how many of these randomization inference loops was the *randomization inference* ate more extreme than the observed ate when you **actually** ran the experiment? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def p_value(ate, ri_dist): \n",
    "    test_vector = np.abs(ri_dist) > np.abs(ate)\n",
    "    test_vector = test_vector.astype('int')\n",
    "    \n",
    "    p_value = test_vector.mean()\n",
    "\n",
    "    return(p_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_value(ate=ate, ri_dist=experiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you want to see each of the sides of this happen; you might consider \n",
    "# reading this version of the same function. \n",
    "#\n",
    "# def p_value(ate, ri_dist): \n",
    "#     if np.sign(ate) == -1: \n",
    "#         negative_vals = (ri_dist < ate).astype('int').mean()\n",
    "#         positive_vals = (ri_dist > -1 * ate).astype('int').mean()\n",
    "#     elif np.sign(ate) == 1: \n",
    "#         negative_vals = (ri_dist < -1 * ate).astype('int').mean()\n",
    "#         positive_vals = (ri_dist > ate).astype('int').mean()\n",
    "#     pvalue = negative_vals + positive_vals\n",
    "#    \n",
    "#     return(pvalue)\n",
    "#\n",
    "# p_value(ate=ate, ri_dist=experiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments_no_effect = experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulate an Experiment with a Large Effect "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have seen that when there is no effect, our Randomization Inference regime does a good job at assigning a high probability of observing an effect size equal to or larger than the ATE we calculate from our particular randomization. \n",
    "\n",
    "Now, let's show that when there is a big effect, our Randomization Inference regime does a good job at assigning a low probability of observing an effect size equal to or larger that the ATE we calculate in our regression. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = make_data(effect_size=25, number_per_group=NUMBER_PER_GROUP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = run_experiment(df, NUMBER_PER_GROUP, False)\n",
    "df.iloc[:10, ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If that is the data that is generated from our actual experiment, how do we generate the average treatment effect? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ate = estimate_ate(df)\n",
    "ate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If that is the estimated ate, what do we get for a histogram of randomization inference? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f'The ate under the assumption of the sharp null is {ri_sim_experiment(df, NUMBER_PER_GROUP)}.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f'The ate under the assumption of the sharp null is {ri_sim_experiment(df, NUMBER_PER_GROUP)}.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question of Understanding \n",
    "\n",
    "1. Are the values under the randomization inference loops (a result of `ri_sim_experiment`) changing? \n",
    "2. Should they be? Why or why not? \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Randomization Inference Under A Strong Effect \n",
    "\n",
    "The previous time that we stepped through a series of randomiation inference loops, we had built the data so that there was *specifically* no effect. And, when we examined the distribution of the treatment effects that could have been possible under:  \n",
    "\n",
    "- The observed data; \n",
    "- Presuming the sharp null was true; \n",
    "\n",
    "We saw that the distribution was centered at zero, and that a *lot* of the simulated distribution was more extreme than the treatment effect that we generated. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(experiments_no_effect)\n",
    "plt.title('The old example')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Current Example \n",
    "Lets simulate these randomization inference loops again, this time with the 25 unit treatment effect. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments = [sim_experiment(df, NUMBER_PER_GROUP) for _ in range(1000)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you plot this histogram: \n",
    "\n",
    "- Where do you think the histogram of randomization inference loops *should* be located? Should it be centered at either: \n",
    "    - Zero\n",
    "    - 25\n",
    "- Why? \n",
    "- Will this histogram have the same general *shape* as the previous histogram? Why or why not? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(experiments)\n",
    "plt.axvline(x=ate, color = 'darkorange')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Produce a statement of the test for how likely a treatment effect of this size is to occur under the presumption of the sharp null hypothesis? (This will be using the `p_value` function that we previously wrote. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_value(ate=ate, ri_dist=experiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
