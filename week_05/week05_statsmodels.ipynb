{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression with Statsmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install stargazer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import statsmodels\n",
    "import statsmodels.formula.api as smf\n",
    "from IPython.core.display import HTML\n",
    "from stargazer.stargazer import Stargazer\n",
    "from statsmodels.stats.anova import anova_lm\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = pd.read_csv('dVote.csv')\n",
    "d.columns = ['in_dex', 'vote_center', 'treat', 'age', 'gender', 'education', 'years_in_us', 'ideology']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the story with the data in here? Are there any strange things that are happening? Maybe in the `ideology` variable that is a 1-7 likert scale? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, we're going to have to do some cleaning of this file to get it ready to go.. \n",
    "\n",
    "1. We don't need the extra index column; could probably fix this at read, but :shrug: \n",
    "2. The treatment probably can be just an int, since it is actually a category indicator. \n",
    "3. Age can be an int \n",
    "4. Gender is stored as an int; but we're going to need to know what those lables are \n",
    "5. Years in us can be an int. \n",
    "6. Ideology can can also be an int. \n",
    "\n",
    "Also, there is some item-level missingness. What would you like to do with it? **Blatently** drop it you say? Well, ok then. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d.drop('in_dex', axis=1, inplace=True)\n",
    "d.dropna(inplace=True)\n",
    "d = d.astype('int') # no inplace method... because consistency "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice nice. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# But, what is actually happening in this data?? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're looking at an experiment where individuals who were born in Mexico but live in the United States were asked about \"Which of these candidates do you think that you are most likely to support, if there were an election between them tomorrow.\" \n",
    "\n",
    "So, we're building an information environment where we can manpulate the features that are available to them to base their decision. \n",
    "\n",
    "Here's what that information environment looks when we place subjects into the 'white' condition: \n",
    "\n",
    "![white](./img/faces1.jpg)\n",
    "\n",
    "What it looks like when we put them into the 'mestizo' condition: \n",
    "\n",
    "![mestizo](./img/faces2.jpg)\n",
    "\n",
    "And, here's what the information environment looks like when we place them into the 'indigenous' condition: \n",
    "\n",
    "![img](./img/faces3.jpg)\n",
    "\n",
    "We also provide people with issue statements that are basically, and intentionally, not that interesting. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we re-level those treatment conditions so that they make a bit more sense. I'm going to subtract one from each of the conditions, so that the control condition (where they get no images) is scored 0, and the other treatment conditions range {1:3}. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d['treat'] = d['treat'] - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using statsmodels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stats models is built for *rapid* development and testing of linear models in python. They leverage some of the great parts of R -- namely the *formulas* style interface for writing out dependend and independent variable concepts. (Yes, you could get this with [patsy](https://patsy.readthedocs.io/en/latest/) formulas, but there is more that we want from statsmodels too. \n",
    "\n",
    "Formulas are structured as: \n",
    "\n",
    "`DV ~ IV` \n",
    "\n",
    "Where the outcome that we're interested in is the IV, and those model features that we've designed are the IVs. \n",
    "\n",
    "In general, we are going to: \n",
    "\n",
    "- Specify a model\n",
    "- Fit that model (with optional args for the vcov structure) \n",
    "- Inspect features of that model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we wanted to know, \"What are the chances that someone votes for the *experimental candidate* and examine if those rates are different in across treatments, how would we structure this? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod1 = smf.ols('vote_center ~ treat', data = d)\n",
    "fit_mod1 = mod1.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(fit_mod1.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this output, we can read into the performance of our model, and interpret coefficeints. In this case, we can know that for each marginal change in the treatment the person was in, there is a 9% increase in the likelihood of voting for the experimental candidate. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Does that make sense...*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Any Treatment? \n",
    "What if people received any form of picture? Did this affect their probability of voting for the center candidate? There are two ways, practically, that we can undertake this question. First, we could include a new feature onto the dataset that is just a `True` if they were in any of the treatment conditions, and `False` if they were in the control condition. This is pretty nice, and makes a persistent feature that we could refer to later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d['any_treat'] = d['treat'] > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The other way would be to make a more ephemeral feature, within the model call, that will *not* persist beyond the model call. On the one hand, this is nice because it keeps from making a large number of transforms against the data; and, also, it doesn't engineer new data that we might not end up using. On the other hand, though, it does increase the time that it takes for a model to run, because we have to compute the vector transform at the time of the model fit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "any_treat1 = smf.ols('vote_center ~ any_treat', data = d).fit()\n",
    "any_treat2 = smf.ols('vote_center ~ C(treat > 0)', data = d).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "any_treat1.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "any_treat2.params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The good news, it seems, is that these produce identical estimates. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Factors of Treatment \n",
    "Of course, the *marginal* interpretation of our treatment conditions was incorrect. Those are actually categorical variables that we assigned to people, not a numeric feature that varries with consistent difference between levels. \n",
    "\n",
    "Just like when creating the `any_treat` feature, we could build the new version in a number of ways. If we want to make a permanent feature, we can use `pd.get_dummies()`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummies = pd.get_dummies(d['treat'], prefix='treat')\n",
    "d = pd.concat([d, dummies], axis=1)\n",
    "\n",
    "mod_dummies_1 = smf.ols('vote_center ~ 1 + treat_1 + treat_2 + treat_3', data = d).fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But, we can also do this work using the `C` operator in the formula notation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod_dummies_2 = smf.ols('vote_center ~ 1 + C(treat)', data = d).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod_dummies_1.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod_dummies_2.params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wait, aren't those numeric variables in the first call? Aren't they factor variables in the second? **Yes**. What about the dummy variables being coded as 0/1 and as True/False makes this possible? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Can we improve this with covariates? \n",
    "\n",
    "One of the claims that we made before is that covariates that are predictive of outcomes should improve the fit of the model, without changing measurably the cofficient that we estimate for the model. Let's see if this works. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod3 = smf.ols('vote_center ~ 1 + C(treat)', data = d).fit()\n",
    "mod4 = smf.ols('vote_center ~ 1 + C(treat) + C(gender) + C(ideology)', data = d).fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look to see what, if anything changes beween the two models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sg = Stargazer([mod3, mod4]).render_html()\n",
    "HTML(sg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a *lot* of levels in that ideology feature; rather than understanding whether any one level of those features is useful in explaining the outcome, what if we wanted to know instead whether *all* the levels of that entire feature were useful? \n",
    "\n",
    "Enter the f-test. Here, we can use the `statsmodels.stats.anova.anova_lm` method to look at whether restricting *all* of the levels of a feature to be zero (that is, requiring that there is no effect of that parameter) makes the model predict the outcome measurably (i.e. significantly) worse than letting those parameters be free. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statsmodels.stats.anova.anova_lm(mod4)\n",
    "# yep, there's a huge error that gets thrown; what I read about it suggests that this is not a concern for us. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we read this, there isn't a lot to suggest that *any* of these features, on their own, are predictive of outcomes. One thing to keep in mind is that: \n",
    "\n",
    "- Features that we *design* to be causal, retain that in these estimates. \n",
    "- Features that we **do not** *design* to be causal don't pick up any causal flavor just because we've put them through a regression. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting with Heteroskedastic Consistent (Robust) Standard Errors \n",
    "David gives a great justification for why \"robust\" standard errors are well-justified in our data. And, implementing these with `statsmodels` borders on being trivial. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have a model that has already been fit, you use a function against the model object to pull the `HC1` errors from that model object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statsmodels.regression.linear_model.RegressionResults.HC1_se(mod3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, more common in practice is to call for the robust standard errors at the time that you're fitting the model. On the one hand, this does remove your ability to pull off *standard* standard errors, but on the other hand, you probably want robust SEs anyways. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "robust_mod3 = smf.ols('vote_center ~ 1 + C(treat)', data = d).fit(cov_type = 'HC1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sg = Stargazer([mod3, robust_mod3]).render_html()\n",
    "HTML(sg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see in this set, there is very little change in the estimates of the SEs; because the data is relatively well behaved across the parameter set. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
